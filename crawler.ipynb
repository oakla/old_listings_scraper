{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup,\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "__url_base = \"https://www.oldlistings.com.au/real-estate/VIC/\"\n",
    "__output_dir = \"output\"\n",
    "__cache_dir = 'cache'\n",
    "__from_cache_file = True\n",
    "__cached_html_filename = \"\"\n",
    "__data_output_file = 'data.json'\n",
    "__targets = 'input/targets.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions - Input Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(address, beds=1, page=1):\n",
    "    # bespoke url elements\n",
    "    address_components = {}\n",
    "    add_arr = address.split()\n",
    "    address_components[\"street\"] = add_arr[1]\n",
    "    address_components[\"suburb\"] = \"\".join(filter(str.isalpha, add_arr[3]))\n",
    "    address_components[\"postcode\"] = add_arr[-1]\n",
    "\n",
    "    return (__url_base \n",
    "        + address_components[\"suburb\"] + '/' \n",
    "        + address_components[\"postcode\"] + '/' \n",
    "        + 'rent/' \n",
    "        + str(page) + '/' \n",
    "        + str.upper(address_components[\"street\"]) \n",
    "        + ':bedmax:' + str(beds))\n",
    "\n",
    "def get_soup(source):\n",
    "    if __from_cache_file: # get soup from cache\n",
    "        filename = make_filename_friendly(source)\n",
    "        global __cached_html_filename \n",
    "        __cached_html_filename = filename\n",
    "        try:\n",
    "            with open(make_html_cache_outpath(filename), 'r') as fp:\n",
    "                return BeautifulSoup(fp.read())\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"error: 'File Not Found' in get_soup() from cached file option\\n\")\n",
    "\n",
    "    else:\n",
    "        html = requests.get(source).text\n",
    "        cache_html(make_filename_friendly(source), html)\n",
    "        return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions - Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slashs(s):\n",
    "    return re.sub('/' , '_', s)\n",
    "\n",
    "def replace_commas(s):\n",
    "    return re.sub(' *, *',' - ', s)\n",
    "\n",
    "def rm_all_invalid_file_name_chars(s):\n",
    "    return re.sub('[^\\w\\s_.-]', '',s)\n",
    "\n",
    "def make_filename_friendly(s):\n",
    "    s = replace_commas(s)\n",
    "    s = replace_slashs(s)\n",
    "    s = rm_all_invalid_file_name_chars(s)\n",
    "    return s\n",
    "\n",
    "def make_outpath_string(directory, filename):\n",
    "    return os.path.join(directory, make_filename_friendly(filename))\n",
    "\n",
    "def make_html_cache_outpath(filename):\n",
    "    return make_outpath_string(__cache_dir, filename + '.html')\n",
    "\n",
    "def write_line_to_file(s):\n",
    "    out = make_outpath_string(__output_dir,'out.txt')\n",
    "    with open(out, 'a') as f:\n",
    "        f.write(s)\n",
    "\n",
    "def cache_html(file_name, content):\n",
    "    outpath = make_html_cache_outpath(file_name)\n",
    "\n",
    "    # *** CANCELLED VERSION NUMBER FEATURE OF CACHED FILES\n",
    "    #       Problem was that read function did not know filename of\n",
    "    #       the most recent file. Must implement the endowment of this\n",
    "    #       ability in reader function first.\n",
    "    #\n",
    "    # outpath = ''\n",
    "    # version = '0'\n",
    "    # while(True):\n",
    "    #     outpath = make_html_cache_outpath(file_name + '_' + version)\n",
    "    #     if(not os.path.exists(outpath)):\n",
    "    #         break\n",
    "\n",
    "    #     version = str(int(version) + 1)\n",
    "    \n",
    "    write_file(outpath, content)\n",
    "\n",
    "def write_file(path, content):\n",
    "# consider using the following next time \n",
    "# (https://stackoverflow.com/questions/273192/how-can-i-safely-create-a-nested-directory-in-python)\n",
    "    # from pathlib import Path\n",
    "    # Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n",
    "    print(\"\\tSaving '\" + path +\"'\")\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    with open(path, 'w') as fp:\n",
    "        fp.write(content)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Search Criteria\n",
    "#### Get targets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_targets():  \n",
    "    rtn_dict = {}\n",
    "\n",
    "    try:\n",
    "        with open(__targets) as file:\n",
    "            while (line := file.readline().rstrip()):\n",
    "                t = line.split(';')\n",
    "                try:\n",
    "                    nras = t[3].lower().strip() in ['true', 'yes', 'y', 't']\n",
    "                except:\n",
    "                    nras = False\n",
    "                try:\n",
    "                    beds = int(t[2].strip())\n",
    "                except:\n",
    "                    beds = 1\n",
    "                rtn_dict[t[0]] = {\"price\": int(t[1]), \"beds\": beds, \"nras\": nras}\n",
    "    except FileNotFoundError:\n",
    "        print('file not found')\n",
    "\n",
    "    return rtn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_listing_in_soup(address, beds):\n",
    "    print(\"\\tsearching website\")\n",
    "    page = 1\n",
    "    while(True):\n",
    "        number = address.split(' ')[0]\n",
    "        url = get_url(address=address, beds=beds, page=page)\n",
    "        soup = get_soup(url)\n",
    "        if \"An Error has occurred\" in soup.find(class_=\"sub-page-h1\").strings:\n",
    "            print(\"Listing wasn't found\")\n",
    "            break\n",
    "\n",
    "        print('\\t\\t' + soup.find(class_ = \"sub-page-h2\").get_text().strip())\n",
    "\n",
    "        listing_name = soup.find(string=re.compile(number))\n",
    "        if(listing_name != None):\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        if page > 10:\n",
    "            print(\"page is getting pretty high: {0}\".format(page))\n",
    "            break\n",
    "        if not __from_cache_file:\n",
    "            time.sleep(3)\n",
    "\n",
    "    old_listing = listing_name.parent.parent.parent\n",
    "    \n",
    "    # print(\"found \" + listing_name)\n",
    "    # print(old_listing.prettify())\n",
    "    # print([x for x in listing_name.stripped_strings])\n",
    "    # cache_html(\"listing - \"\n",
    "    #  + \"\".join(\n",
    "    #      [x for x in listing_name.stripped_strings]), \n",
    "    #      old_listing.prettify())\n",
    "    cache_html(\"00-listing - \"\n",
    "     + listing_name.get_text(), \n",
    "         old_listing.prettify())\n",
    "    print(\"\\tFound listing\")\n",
    "    return old_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_listing(soup):\n",
    "    past_price_list = soup.find(class_=re.compile('hist')).find('ul')\n",
    "    price_date_pairs = {}\n",
    "    for child in past_price_list.find_all(\"li\"):\n",
    "        strings = [x for x in child.strings]\n",
    "        date = str(strings[0]).strip()\n",
    "        dt = datetime.strptime(str(date), '%B %Y')\n",
    "        price_date_pairs[dt.strftime('%Y-%m-%d')] = re.sub('[^0-9]','',strings[1])\n",
    "    print(\"\\tgot price-date pairs\")\n",
    "\n",
    "    return price_date_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from html cache\n",
      "Target is ('317/4 Acacia Place, Abbotsford, Vic 3067', {'price': 290, 'beds': 1, 'nras': True})\n",
      "\tsearching website\n",
      "\t\tDisplaying 1 to 50 of 287 Old Real Estate Listings*\n",
      "\t\tDisplaying 51 to 100 of 287 Old Real Estate Listings*\n",
      "\t\tDisplaying 101 to 150 of 287 Old Real Estate Listings*\n",
      "\t\tDisplaying 151 to 200 of 287 Old Real Estate Listings*\n",
      "\tSaving 'cache\\00-listing - 317_4 ACACIA PLACE - ABBOTSFORD.html'\n",
      "\tFound listing\n",
      "\tgot price-date pairs\n"
     ]
    }
   ],
   "source": [
    "targets = load_targets()\n",
    "target_results_list = []\n",
    "if __from_cache_file:\n",
    "    print(\"Scraping from html cache\")\n",
    "else:\n",
    "    print(\"Scraping from web\")\n",
    "\n",
    "for target in targets.items():\n",
    "    print(\"Target is \" + str(target))\n",
    "    address,features = target\n",
    "    \n",
    "    # make_url()\n",
    "    url = get_url(address, features[\"beds\"])\n",
    "    write_line_to_file(url)\n",
    "    # get_soup()\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # find_listing in soup()\n",
    "    listing_soup = find_listing_in_soup(address, features[\"beds\"])\n",
    "    \n",
    "    # extract_data()\n",
    "    price_date_pairs = extract_data_from_listing(listing_soup)\n",
    "\n",
    "    # TODO: Interpolate price peak amount and date\n",
    "    #       include in output data \n",
    "    # interpolate_peak_price()\n",
    "\n",
    "    # TODO: Create results object to append to results list.\n",
    "    #       Write each item in the results list to output file.\n",
    "    # target_results_list.append(results)\n",
    "\n",
    "    # write_to_file()\n",
    "    data_output_path = make_outpath_string(__output_dir, __data_output_file)\n",
    "    with open(data_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({address:(price_date_pairs,features)}, \n",
    "                    f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
